+++
title = "Introducing rv 0.8.0"
date = 2019-12-20
template = "post.html"
draft = false

[taxonomies]
categories = ["news"]

[extra]
author = "Baxter Eaves"
subheading = "A rust crate for building probabilistic programming tools"
image = "dunes-1.jpg"
theme = "light-transparent"
front_page = false
+++

RV is a statistical toolbox for the Rust programming language designed for our work in probabilistic AI and machine learning. In this post I'll discuss the history of RV and the Rust programming language at Redpoll, RV's design principles, and walk through a few simple examples.

# Rust at Redpoll

At Redpoll, we use [Rust]() for everything in the back end including our AI and services. Safety is our selling point and what's not to love about a safe, performant modern language with a great type system. Well, there's the common problem with all young languages: lack of libraries. We're a research organization, so we don't use much off-the-shelf machine learning code in our product, but still, coming from a python background it sure hurts not to be able to drop [scipy]() and [numpy]() into a project for easy access to scientific computing. What hurt most for the type of machine learning we do was the lack of statistical tools. If I was going to use Rust I'd have to write my own. In a previous role, I had written some probabilistic programming tools in Scala (I don't like Scala for production machine learning), so I had experience with the [Breeze library](). I generally had a good experience with Breeze, so as a Rust newby, I thought I'd just make Breeze in Rust. But as I became more familiar with Rust, the thing I was building became more removed. 

# RV

RV &mdash; which stands for **R**andom **V**ariables &mdash; is a crate for random variables in rust. The design stems from a Björk quote that has influenced how I think when I create things:

> When I did "Debut" I thought, 'OK, I've pleased enough people, I'm gonna get really selfish.' And I never sold as many records as with "Debut". So, I don't know, it seems the more selfish I am, the more generous I am. I'm not going to pretend I know the formula. I can only please myself.

And this is the development philosophy of RV. Selfishness. It should do what we want it to do. But luckily we want it to do a lot and be easy to use.

## Probability distributions

The first ingredient of any probabilistic programming toolbox is probability distributions. In RV 0.8.0 we've implemented 30 distributions. Distributions have different functionality depending on which traits they have implemented. At the minimum, each distribution must implement the `Rv` trait, which allows you to do the bare essentials to be useful in Monte Carlo:

```rust
use rv::traits::Rv;
use rv::dist::Gaussian;

let gauss = Gaussian::standard();
```

We can draw values,

```rust
let mut rng = rand::thread_rng();

let x: f64 = gauss.draw(&mut rng);
```

and we can compute likelihoods.

```rust
let fx = gauss.f(&x);
println!("f({}) = {}", x, fx);

// f(-0.41559754609242056) = 0.3659351330603808
```

The Rv trait has default implementations of `sample`, which draws many values into a vector; and `ln_f`, which is the log likelihood.

We can also compute the distributions statistics like mean, median, mode, etc:

```rust
use rv::traits::{Mean, Median, Mode, Variance};

let mean: f64 = gauss.mean().unwrap();
let median: f64 = gauss.median().unwrap();
let mode: f64 = gauss.mode().unwrap();
let variance: f64 = gauss.variance().unwrap();

println!("{} mean: {}", gauss, mean);
println!("{} median: {}", gauss, median);
println!("{} mode: {}", gauss, mode);
println!("{} variance: {}", gauss, variance);

// N(μ: 0, σ: 1) mean: 0
// N(μ: 0, σ: 1) median: 0
// N(μ: 0, σ: 1) mode: 0
// N(μ: 0, σ: 1) variance: 1
```

You'll notice a lot of unwrapping. These methods return Option because some of these quantities are not always defined. For example:

```rust
use rv::dist::InvGamma;

let ig_var = InvGamma::new(3.0, 1.0).unwrap();

// The inverse Gamma variance is undefined if the first
// parameter, shape, is less than 2.
let ig_no_var = InvGamma::new(1.0, 1.0).unwrap();

assert!(ig_no_var.variance().is_none());
assert!(ig_var.variance().is_some());
```

## Information theory

```rust
use rv::traits::{Entropy, KlDivergence};

// This is continuous (differential) entropy
let h = gauss.entropy();
println!("{} entropy: {}", gauss, h);

// KL Divergence
let other = Gaussian::new(2.0, 1.0).unwrap();
let kl = gauss.kl(&other);
println!("KL({} | {}): {}", gauss, other, kl);

// N(μ: 0, σ: 1) entropy: 1.4189385332046727
// KL(N(μ: 0, σ: 1) | N(μ: 2, σ: 1)): 2
```

## Conjugate Inference

```rust
use rv::dist::{Beta, Bernoulli};
use rv::data::DataOrSuffStat;
use rv::traits::ConjugatePrior;


let flips = vec![true, false, false, false];
let prior = Beta::jeffreys();
let xs: DataOrSuffStat<bool, Bernoulli> = DataOrSuffStat::Data(&flips);

let posterior: Beta = prior.posterior(&xs);
println!("Coin weight Posterior: {}", posterior);

let ml_weight: f64 = posterior.mode().unwrap();
println!("Coin weight ML: {}", ml_weight);

let var_weight: f64 = posterior.variance().unwrap();
println!("Coin weight variance: {}", var_weight);

// Coin weight Posterior: Beta(α: 1.5, β: 3.5)
// Coin weight ML: 0.16666666666666666
// Coin weight variance: 0.035<Paste>
```

## Conjugate Inference on Custom Type

Some distributions, namely those with support for categorical data, support user-defined data types by way of traits that convert those types to and from a native data type. For example, instead of representing coin flips as `bool`, we can go ahead and define a `Coin` enum. We will need to implement the `Booleable` trait, which requires that our type implements `Copy`.

```rust
use rv::data::Booleable;

// Explicitly represent each variant as a u8
#[repr(u8)]
#[derive(Clone, Copy, PartialEq)]
enum Coin {
    Tails,
    Heads
}

impl Booleable for Coin {
    fn from_bool(heads: bool) -> Coin {
        if heads {
            Coin::Heads
        } else {
            Coin::Tails
        }
    }

    fn try_into_bool(self) -> Option<bool> {
        Some(self == Coin::Heads)
    }
}
```

Now we can re-write the above example more explicitly using `Coin`

```rust
let flips = vec![Coin::Heads, Coin::Tails, Coin::Tails, Coin::Tails];

let prior = Beta::jeffreys();

let xs: DataOrSuffStat<Coin, Bernoulli> = DataOrSuffStat::Data(&flips);

let posterior: Beta = prior.posterior(&xs);
println!("Coin weight Posterior: {}", posterior);

let ml_weight: f64 = posterior.mode().unwrap();
println!("Coin weight ML: {}", ml_weight);

let var_weight: f64 = posterior.variance().unwrap();
println!("Coin weight variance: {}", var_weight);

// Coin weight Posterior: Beta(α: 1.5, β: 3.5)
// Coin weight ML: 0.16666666666666666
// Coin weight variance: 0.035
```

## Error

Constructors validate input unless they are marked with `_unchecked`. 

```rust
use rv::dist::NewGaussianError;

match Gaussian::new(0.0, -3.0) {
    Err(NewGaussianError::SigmaTooLowError) => println!("Expected error"),
    Err(_) => panic!("Unexpected error"),
    Ok(_) => panic!("There should have been an error"),
}

// Expected error
```
Unchecked constructors let whatever you want through. This could lead to panics or undefined behavior.

```rust
let gauss = Gaussian::new_unchecked(0.0, -3.0);

// panics here because the PDF will try to compute ln(-3.0)
let ln_fx = gauss.ln_f(&0.0);
```

# Building bigger things

These are all very simple examples but they hopefully give you a sense of the building blocks of `RV`. In the examples directory, we have built an infinite Gaussian mixture model (an unsupervised clustering and density estimation algorithm), in about 100 line of code.
