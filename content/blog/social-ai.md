+++
title = "Who goes to jail when artificial intelligence fails?"
date = 2020-10-15
template = "post.html"
draft = true

[taxonomies]
categories = ["ai"]

[extra]
author = "Baxter Eaves"
subheading = "The ethics and pitfalls of transferring liability to the end user"
image = "person-walking-on-street-between-buildings-3617457.jpg"
theme = "light-transparent"
front_page = false
+++

People have all sorts of social biases that determine how they learn from and how they trust people. These biases are an implicit contract: Informant will behave a certain way, and if you assume such behavior from Informant, you will learn much faster. But, people also tend to anthropomorphize machines. So if all people have strong social learning biases, and people often humanize machines, it follows that people often assume human social learning behavior to machines.

If we know that people will misunderstand our machines, we know that people will misuse our machines, so we cannot ethically turn them over to people making vital decisions. Training people to correctly understand inhuman machines is infeasible and ineffective. The machine itself must psychologically behave like a person at an algorithmic level. What the machine learns, it must learn like a person; what the machine communicates, it must communicate like a person. Humanistic AI incorporates
