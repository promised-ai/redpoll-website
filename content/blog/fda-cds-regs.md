+++
title = "Training deep neural networks to skirt FDA regulation"
date = 2019-10-08
template = "post.html"
draft = false

[taxonomies]
categories = ["ai", "ethics"]

[extra]
author = "Baxter Eaves"
subheading = "Or: Concerns with weak language in the FDA's guidance on clinical decision support software"
image = "healthcare-hospital-lamp.jpg"
theme = "light-transparent"
+++

The FDA recently released (September 29th 2019) a guidance on the regulation of clinical decision support (CDS) software. The guidance focuses on what software falls under the definition of a "device" and is therefore subject to regulation. In this post, I will discuss how certain terminology could be attacked to avoid regulation, and how the psychology surrounding *decision bases* should impact a software's regulation status. I argue that there being a health care professional between patient and software should is a poor reason not to regulate a software product.

# The FDA Guidance

[The guidance](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software)

# Decision Basis

The primary factor to determining whether a CDS systems is or is not a device is whether it is intended to be the primary decision maker. In order not to be labeled as a device the software must be

> [...] intended to enable [health care professionals] to independently review the basis for the recommendations presented by the software so that they do not rely primarily on such recommendations, but rather on their own judgment, to make clinical decisions for individual patients.

What is a basis? From line 254 of the guidance:

> In order to describe the basis for a recommendation, regardless of the complexity of the software and whether or not it is proprietary, the software developer should describe the underlying data used to develop the algorithm and should include plain language descriptions of the logic or rationale used by an algorithm to render a recommendation.


These are not satisfying constraints on modern AI. To what degree do we have to describe logic or rationale? What is rationale? Is "The patient is 24 year old, so they might enjoy an anticoagulant" sufficient rationale? To what degree does the description have to align with the mathematical workings of the algorithm? Is there a distinction between *algorithm* and *model*? Can I invoke the stochastic gradient descent optimization algorithm to have my software labeled as an unregulated non-device?

If we look at the complex, black box AI we have today, there is really no way to plainly and completely describe the logic. We can say something like "Input data X, Y, and Z were important factors to this flag". Because many of these models are nonsensical, an notion of *important* applies to only one particular model, and does not reflect a human being's intuition of what actually is important, so it could be misleading. An audit trail could be derived following the inputs' activation of various weights in a neural networks, but again, this is nonsensical; it does not help the physician scrutinize the decision.


## Training a machine learning model to skirt regulation

Now we have to worry about how we prevent decision bases from becoming unscrupulous. Now that providing basis is a path to regulation-free sales, and since there is no acceptable definition of what a sufficient basis is, a basis in large part can be manufactured. And it's easy to attack this weak language.

The language is weak because what it means is left to judgement; not put to unambiguous words. A good decision basis is then like pornography: As [Supreme Court Justice Potter Stewart said](https://www.law.cornell.edu/supremecourt/text/378/184) "I know it when I see it". Given any explanation, a practitioner will know whether that explanation is an acceptable decision basis under the guidelines. As developers, we observe the explanations our software generates and the judgment of the end user (acceptable or unacceptable). Now all we need to do is learn the nebulous definition that links explanations to judgements. That unknown definition is an unknown function from inputs (explanations) to output (judgements)... We have a machine learning problem ideal for a black box classification framework.

The attack is simple: use feedback from healthcare professionals (HCPs) and the FDA itself to train a recurrent neural network to avoid regulation. We attach the recurrent neural network to the end of whatever black box is making the recommendations. We do the initial training on bases generated by inexpensive lay people. We then get feedback from HPCs telling us which explanations are acceptable; then retrain. We can then seek approval or feedback from FDA personnel who will certainly point out examples of poor explanation. Our training data are then the lay explanations, HPC feedback, and FDA feedback. If our training data are rich enough, we have trained a neural network to produce explanations that pass scrutiny but that are completely brittle and senseless. If we've done really well, we also have explanations that HPCs favor, which means we're selling software HPCs favor.

We've trained a stupid brittle system that HPCs love. And because our stupid brittle system is not *intended* to be used as a primary decision-maker, when it goes wrong, it's the HPC who takes the fall.

## Becoming white noise: a lesser concern

People feel better about decisions that are explained to them. This is why Netflix recommends movie X based on your interest in movies Y; and why Amazon recommends product Z to *users like you*. These explanations are not designed to help us make more confident decisions, they are designed to help us make the decision Netflix and Amazon want. This is a problem for CDS. The FDA's intention for CDS decision bases are to help health care professionals appropriately trust and scrutinize decisions, but they could target human biases that tend to increase adherence to machine recommendations. We Netflix and Amazon consumers have realized explanations are a manipulation, and we often just tune them out. Another problem for CDS. How do we prevent decision bases from becoming white noise? How do we prevent decision bases from being useless?

## Toward a safe definition of decision basis



# Wrapping up

A software's regulation status in wrapped up in whether it provides sufficient basis for its decision. A basis is an explanation. Machine explanations are not necessarily auditable nor are they necessarily sensible. Given enough approval/disapproval from regulators, researchers would be trained in how to train black box AI to generate superficial bases that pass clinical scrutiny but are brittle in the real world. There are two solutions:

1. Develop a definition of decision basis that revolve around linking inputs to output via the full structure of the machine knowledge (hard).
2. Label all CDS software as a device and regulate it (easy).

# Asides

1. Complacency is a big problem in safety sensitive domains. When the airport security scan bags for weapons, many more bags have nothing than something. After a while agents expect to see *nothing*, so it becomes harder to see *something*, so things get overlooked. To combat this, some scanners project false contraband in the imagery. This technique is called [threat image projection](https://www.rapiscansystems.com/en/products/rapiscan-threat-image-projection). The agents tag the projection, so the machine knows they're paying attention, and they move on.
